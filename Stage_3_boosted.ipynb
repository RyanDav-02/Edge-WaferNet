{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b912679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ==========================================\n",
    "# 1. REPRODUCIBILITY & COMPATIBILITY\n",
    "# ==========================================\n",
    "# Fixing the seed ensures the same result every time you run it\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import pandas.core.indexes as indexes\n",
    "sys.modules['pandas.indexes'] = indexes\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING & ENHANCED PREPROCESSING\n",
    "# ==========================================\n",
    "def load_and_preprocess():\n",
    "    file_path = 'LSWMD.pkl'\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "        df = pd.DataFrame(data)\n",
    "        df['failureType'] = df['failureType'].apply(\n",
    "            lambda x: x[0][0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else 'none'\n",
    "        )\n",
    "        df_defects = df[df['failureType'] != 'none'].copy()\n",
    "    else:\n",
    "        # Synthetic Fallback\n",
    "        test_data = {'waferMap': [np.random.randint(0, 3, (26, 26)) for _ in range(1000)],\n",
    "                     'failureType': [np.random.choice(['Center', 'Donut', 'Scratch']) for _ in range(1000)]}\n",
    "        df_defects = pd.DataFrame(test_data)\n",
    "\n",
    "    label_map = {val: i for i, val in enumerate(df_defects['failureType'].unique())}\n",
    "    df_defects['label_id'] = df_defects['failureType'].map(label_map)\n",
    "    \n",
    "    processed_images = []\n",
    "    for x in df_defects['waferMap']:\n",
    "        # Applied a slightly stronger median filter for cleaner feature extraction\n",
    "        denoised = ndimage.median_filter(x, size=3) \n",
    "        resized = tf.image.resize(denoised[:, :, np.newaxis], (28, 28)).numpy()\n",
    "        processed_images.append(resized)\n",
    "    \n",
    "    X = np.array(processed_images).astype('float32') / 2.0\n",
    "    y = tf.keras.utils.to_categorical(df_defects['label_id'])\n",
    "    return X, y, label_map\n",
    "\n",
    "X, y, label_map = load_and_preprocess()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==========================================\n",
    "# 3. BOOSTED EDGE-AI ARCHITECTURE\n",
    "# ==========================================\n",
    "\n",
    "def build_boosted_model(num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        \n",
    "        # Increased filters slightly to capture more complex failure geometries\n",
    "        layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3), # Higher dropout to prevent overfitting during 20 epochs\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_boosted_model(len(label_map))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ==========================================\n",
    "# 4. LEARNING RATE SCHEDULER & TRAINING\n",
    "# ==========================================\n",
    "# This callback reduces the learning rate when the model stops improving\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n",
    "\n",
    "print(\"\\nStarting Boosted Training (20 Epochs)...\")\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[lr_reducer])\n",
    "\n",
    "# ==========================================\n",
    "# 5. FINAL QUANTIZATION & EXPORT\n",
    "# ==========================================\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('boosted_defect_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# ==========================================\n",
    "# 6. RESULTS & VISUALIZATION\n",
    "# ==========================================\n",
    "def plot_boosted_results(history, model, X_test, y_test, label_map):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy'); ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_title('Boosted Accuracy'); ax1.legend()\n",
    "    \n",
    "    ax2.plot(history.history['loss'], label='Train Loss'); ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Boosted Loss'); ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "    plt.title('Final Confusion Matrix'); plt.show()\n",
    "    print(classification_report(y_true, y_pred, target_names=list(label_map.keys())))\n",
    "\n",
    "plot_boosted_results(history, model, X_test, y_test, label_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
